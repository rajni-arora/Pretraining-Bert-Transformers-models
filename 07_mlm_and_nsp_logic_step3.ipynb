{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajni-arora/Pretraining-Bert-Transformers-models/blob/main/07_mlm_and_nsp_logic_step3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JhwBw7FoBlP"
      },
      "source": [
        "# Training With MLM and NSP\n",
        "\n",
        "BERT was originally trained using both MLM *and* NSP. So far, we've dived into how we can use each of those individually - but not together. In this notebook, we'll explore how.\n",
        "\n",
        "First, we'll start by initializing everything we need. This time, rather than using a `BertForMaskedLM` or `BertForNextSentencePrediction` class, we use the `BertForPreTraining` class - which includes both a MLM head, and an NSP head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQbuuRj1oBlU",
        "outputId": "a5eafdee-e699-43af-98ae-d2657c3d36ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForPreTraining\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "\n",
        "text = (\"After Abraham Lincoln won the November 1860 presidential [MASK] on an \"\n",
        "        \"anti-slavery platform, an initial seven slave states declared their \"\n",
        "        \"secession from the country to form the Confederacy.\")\n",
        "text2 = (\"War broke out in April 1861 when secessionist forces [MASK] Fort \"\n",
        "         \"Sumter in South Carolina, just over a month after Lincoln's \"\n",
        "         \"inauguration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9T7uQ2joBlY"
      },
      "source": [
        "We tokenize just as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6td8xN25oBlY",
        "outputId": "69fbdf4e-b401-47ea-c82b-2df52bc7aed2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,   103,\n",
              "          2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,\n",
              "          6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,\n",
              "          1996, 18179,  1012,   102,  2162,  3631,  2041,  1999,  2258,  6863,\n",
              "          2043, 22965,  2923,  2749,   103,  3481,  7680,  3334,  1999,  2148,\n",
              "          3792,  1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055,\n",
              "         17331,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(text, text2, return_tensors='pt')\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhddVpXSoBlZ"
      },
      "source": [
        "And process those `inputs` as we did before too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMGkWCjqoBlZ"
      },
      "outputs": [],
      "source": [
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImHJzNmHoBlZ",
        "outputId": "638a0c4d-8fd1-4417-cb06-030ed7f43d4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['prediction_logits', 'seq_relationship_logits'])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLGt7qYqoBla"
      },
      "source": [
        "We will find that we now return two output tensors:\n",
        "\n",
        "* *prediction_logits* for our predicted output tokens - which we will use for calculating MLM loss.\n",
        "\n",
        "* *seq_relationship_logits* for our predicted `IsNextSentence` or `NotNextSentence` classifications, which we can use for calculating NSP loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrCYyga_oBla",
        "outputId": "ed36c2ee-265a-4fc5-b874-bc7288656412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ -8.2231,  -8.0797,  -8.1794,  ...,  -7.3005,  -7.2905,  -5.0078],\n",
              "         [-12.6802, -12.6025, -12.7861,  ..., -12.2099, -11.7521,  -8.9502],\n",
              "         [ -6.1791,  -6.3236,  -5.8000,  ...,  -6.0132,  -6.1775,  -4.6381],\n",
              "         ...,\n",
              "         [ -1.7799,  -1.5980,  -1.7029,  ...,  -1.2014,  -1.2108,  -6.9480],\n",
              "         [-14.3935, -14.4690, -14.3760,  ..., -11.7672, -11.9389, -10.8475],\n",
              "         [-13.9112, -14.0683, -13.9222,  ..., -11.3252, -11.7293, -10.5214]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.prediction_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e83IQ8sgoBlb",
        "outputId": "a5887c0b-2c38-41aa-be1c-7dd898211198"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 6.0843, -5.6813]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.seq_relationship_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP2sBZJ-oBlb"
      },
      "source": [
        "But how to we return the *loss* tensor? We need to add labels, both for the MLM head, and the NSP head.\n",
        "\n",
        "We have two additional input labels for our model:\n",
        "\n",
        "* *labels* for our MLM.\n",
        "* *next_sentence_label* for NSP.\n",
        "\n",
        "All we need to do is fill both of these. First let's fill in the *\\[MASK\\]* tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSUxG_kroBlc"
      },
      "outputs": [],
      "source": [
        "text = (\"After Abraham Lincoln won the November 1860 presidential election on an \"\n",
        "        \"anti-slavery platform, an initial seven slave states declared their \"\n",
        "        \"secession from the country to form the Confederacy.\")\n",
        "text2 = (\"War broke out in April 1861 when secessionist forces attacked Fort \"\n",
        "         \"Sumter in South Carolina, just over a month after Lincoln's \"\n",
        "         \"inauguration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEmJ-pXMoBlc"
      },
      "source": [
        "And tokenize our text again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP1ByGm7oBlc"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(text, text2, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dEXcpLFoBlc"
      },
      "source": [
        "This time, we must `clone` our *input_ids* tensor to create a new *labels* tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m85Iw6FUoBld"
      },
      "outputs": [],
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzzQaEVUoBld"
      },
      "source": [
        "Now we can go ahead and mask *'election'* and *'attacked'* in the *input_ids* tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7uuayr1oBld",
        "outputId": "311bbc16-7a63-4b1d-bc1b-01c13f0aae8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,  2602,\n",
              "          2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,\n",
              "          6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,\n",
              "          1996, 18179,  1012,   102,  2162,  3631,  2041,  1999,  2258,  6863,\n",
              "          2043, 22965,  2923,  2749,  4457,  3481,  7680,  3334,  1999,  2148,\n",
              "          3792,  1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055,\n",
              "         17331,  1012,   102]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs.input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLxMroN0oBle",
        "outputId": "b3b7c082-42fb-4f86-a63a-3b62973869e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,   103,\n",
              "          2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,\n",
              "          6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,\n",
              "          1996, 18179,  1012,   102,  2162,  3631,  2041,  1999,  2258,  6863,\n",
              "          2043, 22965,  2923,  2749,   103,  3481,  7680,  3334,  1999,  2148,\n",
              "          3792,  1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055,\n",
              "         17331,  1012,   102]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs.input_ids[0, [9, 44]] = 103\n",
        "inputs.input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Th7wRJoBle"
      },
      "source": [
        "And now we just need to add our *next_sentence_label* tensor, which is a simple single value `LongTensor` like in the previous NSP sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1Do___xoBle"
      },
      "outputs": [],
      "source": [
        "inputs['next_sentence_label'] = torch.LongTensor([0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6YVcsKVoBle"
      },
      "source": [
        "Now our `inputs` are ready for processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMX1JaTYoBlf",
        "outputId": "ece810e5-b84d-4a86-a562-133f39789f80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,   103,\n",
              "          2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,\n",
              "          6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,\n",
              "          1996, 18179,  1012,   102,  2162,  3631,  2041,  1999,  2258,  6863,\n",
              "          2043, 22965,  2923,  2749,   103,  3481,  7680,  3334,  1999,  2148,\n",
              "          3792,  1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055,\n",
              "         17331,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,  2602,\n",
              "          2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,\n",
              "          6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,\n",
              "          1996, 18179,  1012,   102,  2162,  3631,  2041,  1999,  2258,  6863,\n",
              "          2043, 22965,  2923,  2749,  4457,  3481,  7680,  3334,  1999,  2148,\n",
              "          3792,  1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055,\n",
              "         17331,  1012,   102]]), 'next_sentence_label': tensor([0])}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia63YBHLoBlf",
        "outputId": "ff99c364-e70a-4fa8-f67e-79a606a72f90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['loss', 'prediction_logits', 'seq_relationship_logits'])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oj39pNLoBlf"
      },
      "source": [
        "And now we can see that we've returned another tensor, our *loss*!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI0geEcdoBlf",
        "outputId": "6ed6e2cb-803d-4af9-dc03-be8dfd6c8790"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0767, grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4usHiLEXoBlg"
      },
      "source": [
        "We can then go ahead and use this loss when fine-tuning our models using both MLM and NSP."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}